{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608f0ebe",
   "metadata": {},
   "source": [
    "# Setup:\n",
    "\n",
    "1. Ensure everything in `System/README.md` is done\n",
    "2. Create a file `System/.env` that has API_KEY={Gemini_API_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25058abd",
   "metadata": {},
   "source": [
    "Sources: This would be near impossible without them\n",
    "- Article: https://codeawake.com/blog/postgresql-vector-database\n",
    "- RAG from scratch: https://github.com/ruizguille/rag-from-scratch\n",
    "- Personal project: https://github.com/Lingotech-Davis/NewsDashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4a55f",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- Learning: This is just me going through tutorial code & combining different aspects from the above 3 sources\n",
    "- Final System: Has all the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e015e2d",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea958038",
   "metadata": {},
   "source": [
    "For now we will use a simple placeholder for the actual sources\n",
    "- Note: When we run benchmarks we should have a set of sources anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ca9e851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Barack_Obama\n",
      "Barack Obama\n",
      "Barack Hussein Obama II (born August 4, 1961) is an American politician who served as the 44th presi...\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "# Parameters\n",
    "query = \"Obama\"\n",
    "k = 5\n",
    "\n",
    "# Get titles\n",
    "titles = wikipedia.search(query, results=k)\n",
    "\n",
    "# Load page\n",
    "page = wikipedia.page(titles[0], auto_suggest=False)\n",
    "\n",
    "# Useful attributes\n",
    "print(page.url)\n",
    "print(page.title)\n",
    "print(page.content[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72464660",
   "metadata": {},
   "source": [
    "Set up database stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14480a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, this is an example \n",
      " of a document\n",
      "{'Author': 'Chengyi', 'Date': '1/29/26'}\n",
      "[{'text': 'Hello, this is an example of a document', 'embedding': [39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]}]\n"
     ]
    }
   ],
   "source": [
    "# A class that defines a document as:\n",
    "# - text: full raw text\n",
    "# - metadata: a dictionary\n",
    "# - chunks: a list of chunks from the raw text ['text'] and ['embedding']\n",
    "class RAGDocument():\n",
    "  def __init__(self, text: str, metadata: dict[str]):\n",
    "    self.text = text\n",
    "    self.metadata = metadata\n",
    "    self.chunks = \"Not chunked yet\"\n",
    "  def _to_chunks(self, max_tokens=512, overlap=25):\n",
    "    tokens = self.text.split()\n",
    "    self.chunks = []\n",
    "    start_idx = 0\n",
    "    while start_idx < len(tokens):\n",
    "      end_idx = start_idx + max_tokens\n",
    "      curr_chunk = tokens[start_idx:end_idx]\n",
    "      self.chunks.append({\"text\": \" \".join(curr_chunk),\n",
    "                          \"embedding\": \"Not embedded yet\"})\n",
    "      start_idx += max_tokens - overlap\n",
    "  def _to_embeddings(self, embedding_model=None):\n",
    "    for index, chunk in enumerate(self.chunks):\n",
    "      self.chunks[index][\"embedding\"] = embedding_model.encode(chunk['text'])\n",
    "  def chunkify(self, max_tokens=512, overlap=25, embedding_model=None):\n",
    "    if overlap >= max_tokens:\n",
    "      raise ValueError(\"Overlap must be smaller than max_tokens\")\n",
    "    if not embedding_model:\n",
    "      raise ValueError(\"Embedding model not specified\")\n",
    "    self._to_chunks(max_tokens, overlap)\n",
    "    self._to_embeddings(embedding_model=embedding_model)\n",
    "\n",
    "\n",
    "# Testing it out\n",
    "class testEmbeddingModel():\n",
    "  def __init__(self):\n",
    "    return\n",
    "  def encode(self, text):\n",
    "    return [len(text)] * 384\n",
    "doc = RAGDocument(\"Hello, this is an example \\n of a document\", metadata={\"Author\": \"Chengyi\", \"Date\": \"1/29/26\"})\n",
    "doc.chunkify(embedding_model=testEmbeddingModel())\n",
    "print(doc.text)\n",
    "print(doc.metadata)\n",
    "print(doc.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e195f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'text': \"Python is a high-level programming language known for its simplicity and readability. It's widely used in web development, data science, and artificial intelligence.\", 'metadata': {'topic': 'programming', 'author': 'Test User', 'language': 'Python'}, 'score': 1.0}\n",
      "Result: {'text': 'Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data. Popular frameworks include TensorFlow, PyTorch, and scikit-learn.', 'metadata': {'topic': 'AI/ML', 'author': 'Test User', 'subtopic': 'machine learning'}, 'score': 1.0}\n",
      "Result: {'text': 'PostgreSQL is a powerful open-source relational database system. It supports advanced features like JSONB data types, full-text search, and vector similarity search through extensions.', 'metadata': {'topic': 'databases', 'author': 'Test User', 'db_type': 'PostgreSQL'}, 'score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\n",
    "from sqlalchemy import Text, select\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker\n",
    "import numpy as np\n",
    "\n",
    "# 1. Initialize Vector class\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "class VectorDocument(Base):\n",
    "    __tablename__ = 'vectors'\n",
    "\n",
    "    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)\n",
    "    text: Mapped[str] = mapped_column(Text)\n",
    "    vector = mapped_column(Vector(384))\n",
    "    metadata_: Mapped[dict | None] = mapped_column('metadata', JSONB)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Vector(id={self.id}, text={self.text[:50]}..., metadata={self.metadata_})'\n",
    "\n",
    "# 2. Prepare to create a database\n",
    "username = 'username'\n",
    "password = 'password'\n",
    "DB_URL = f'postgresql+asyncpg://{username}:{password}@localhost:5432/edgerag_db'\n",
    "engine = create_async_engine(DB_URL)\n",
    "\n",
    "async def db_create():\n",
    "    async with engine.begin() as conn:\n",
    "        await conn.run_sync(Base.metadata.create_all)\n",
    "\n",
    "# 3. Create session\n",
    "Session = async_sessionmaker(engine, expire_on_commit=False)\n",
    "\n",
    "# 4. Initialize adding a document to database\n",
    "async def add_document_to_vector_db(doc: RAGDocument):\n",
    "    doc_chunks = doc.chunks\n",
    "    commit_chunks = []\n",
    "    for doc_chunk in doc_chunks:\n",
    "        commit_chunks.append({\n",
    "            'text': doc_chunk['text'],\n",
    "            'vector': doc_chunk['embedding'],\n",
    "            'metadata_': doc.metadata\n",
    "        })\n",
    "    async with Session() as db:\n",
    "        for commit_chunk in commit_chunks:\n",
    "            db.add(VectorDocument(**commit_chunk))\n",
    "        await db.commit()\n",
    "\n",
    "# 5. Initialize searching the database\n",
    "async def vector_search(query_vector, top_k=3):\n",
    "    async with Session() as db:\n",
    "        query = (\n",
    "            select(VectorDocument.text, VectorDocument.metadata_, VectorDocument.vector.cosine_distance(query_vector).label('distance'))\n",
    "            .order_by('distance')\n",
    "            .limit(top_k)\n",
    "        )\n",
    "        res = await db.execute(query)\n",
    "        result = []\n",
    "        for text, metadata, distance, in res:\n",
    "            result.append({\n",
    "                'text': text,\n",
    "                'metadata': metadata,\n",
    "                'score': 1 - distance\n",
    "            })\n",
    "        return result\n",
    "\n",
    "# Testing it out\n",
    "docs = [\n",
    "    RAGDocument(\n",
    "        text=\"Python is a high-level programming language known for its simplicity and readability. It's widely used in web development, data science, and artificial intelligence.\",\n",
    "        metadata={\"topic\": \"programming\", \"language\": \"Python\", \"author\": \"Test User\"}\n",
    "    ),\n",
    "    RAGDocument(\n",
    "        text=\"Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data. Popular frameworks include TensorFlow, PyTorch, and scikit-learn.\",\n",
    "        metadata={\"topic\": \"AI/ML\", \"subtopic\": \"machine learning\", \"author\": \"Test User\"}\n",
    "    ),\n",
    "    RAGDocument(\n",
    "        text=\"PostgreSQL is a powerful open-source relational database system. It supports advanced features like JSONB data types, full-text search, and vector similarity search through extensions.\",\n",
    "        metadata={\"topic\": \"databases\", \"db_type\": \"PostgreSQL\", \"author\": \"Test User\"}\n",
    "    ),\n",
    "]\n",
    "await db_create()\n",
    "embedding_model = testEmbeddingModel()\n",
    "for doc in docs:\n",
    "    doc.chunkify(max_tokens=512, overlap=2, embedding_model=embedding_model)\n",
    "    await add_document_to_vector_db(doc)\n",
    "results = await vector_search([69]*384, top_k=3)\n",
    "for result in results:\n",
    "    print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9d40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development: Reset\n",
    "async def reset_db():\n",
    "    async with engine.begin() as conn:\n",
    "        await conn.run_sync(Base.metadata.drop_all)\n",
    "        await conn.run_sync(Base.metadata.create_all)\n",
    "\n",
    "await reset_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv('.env')\n",
    "API_KEY = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfb81108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengyi/anaconda3/envs/EdgeRAG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 103/103 [00:01<00:00, 101.79it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI assistant that answers questions about documents in your knowledge base.\n",
    "\"\"\"\n",
    "\n",
    "RAG_PROMPT = \"\"\"\n",
    "Use the following pieces of context to answer the user question.\n",
    "You must only use the facts from the context to answer.\n",
    "If the answer cannot be found in the context, say that you don't have enough information to answer the question and provide any relevant facts found in the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "async def answer_question_with_rag(question):\n",
    "    query_vector = embedding_model.encode(question)\n",
    "    top_chunks = await vector_search(query_vector, top_k=3)\n",
    "    context = '\\n\\n---\\n\\n'.join([chunk['text'] for chunk in top_chunks]) + '\\n\\n---'\n",
    "    user_message = RAG_PROMPT.format(context=context, question=question)\n",
    "    messages = SYSTEM_PROMPT + \" \" + user_message\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents = messages\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a7bb9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have enough information to answer the question as the provided context is empty.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the main challenges in renewable energy adoption?\"\n",
    "response = await answer_question_with_rag(question)\n",
    "print(response.text) # This is an empty response, just for demonstration purposes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44799237",
   "metadata": {},
   "source": [
    "# Final System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b898253",
   "metadata": {},
   "source": [
    "In this example, here is our system:\n",
    "- Retrieval: Wikipedia Articles\n",
    "- Database: Postgres\n",
    "- Embedding model: all-MiniLM-L6-v2\n",
    "- Text generation model: gemini-2.5-flash\n",
    "\n",
    "The parts should be pretty swappable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c4a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv('.env')\n",
    "API_KEY = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab12a96",
   "metadata": {},
   "source": [
    "Define helper classes / items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714cd6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from sqlalchemy import Text\n",
    "from pgvector.sqlalchemy import Vector\n",
    "\n",
    "class RAGDocument():\n",
    "\n",
    "  \"\"\"\n",
    "  A class that initializes a document as:\n",
    "  - text: full raw text\n",
    "  - metadata: a dictionary\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, text: str, metadata: dict[str]):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    _ (above)\n",
    "    - chunks: a list of chunks from the raw text with keys ['text'] and ['embedding']\n",
    "    \"\"\"\n",
    "\n",
    "    self.text = text\n",
    "    self.metadata = metadata\n",
    "    self.chunks = \"Not chunked yet\"\n",
    "  def _to_chunks(self, max_tokens, overlap):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper: reates a list of chunks of size max_tokens + overlap\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = self.text.split()\n",
    "    self.chunks = []\n",
    "    start_idx = 0\n",
    "    while start_idx < len(tokens):\n",
    "      end_idx = start_idx + max_tokens\n",
    "      curr_chunk = tokens[start_idx:end_idx]\n",
    "      self.chunks.append({\"text\": \" \".join(curr_chunk),\n",
    "                          \"embedding\": \"Not embedded yet\"})\n",
    "      start_idx += max_tokens - overlap\n",
    "  def _to_embeddings(self, embedding_model):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper: Encodes the chunk using the embedding model\n",
    "    \"\"\"\n",
    "\n",
    "    for index, chunk in enumerate(self.chunks):\n",
    "      self.chunks[index][\"embedding\"] = embedding_model.encode(chunk['text'])\n",
    "  def chunkify(self, max_tokens=512, overlap=25, embedding_model=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - max_tokens: number of tokens per chunk\n",
    "    - overlap: how much overlap to consider\n",
    "    - embedding_model: a function that has the method encode(text) which returns a vector of size VECTOR_SIZE\n",
    "    \"\"\"\n",
    "\n",
    "    if overlap >= max_tokens:\n",
    "      raise ValueError(\"Overlap must be smaller than max_tokens\")\n",
    "    if not embedding_model:\n",
    "      raise ValueError(\"Embedding model not specified\")\n",
    "    self._to_chunks(max_tokens, overlap)\n",
    "    self._to_embeddings(embedding_model=embedding_model)\n",
    "\n",
    "\n",
    "class Base(DeclarativeBase):\n",
    "\n",
    "    \"\"\"\n",
    "    For defining VectorDocument\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "class VectorDocument(Base):\n",
    "\n",
    "    \"\"\"\n",
    "    For representing a document in the database\n",
    "    \"\"\"\n",
    "\n",
    "    __tablename__ = 'vectors'\n",
    "    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)\n",
    "    text: Mapped[str] = mapped_column(Text)\n",
    "    vector = mapped_column(Vector(384))\n",
    "    metadata_: Mapped[dict | None] = mapped_column('metadata', JSONB)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Vector(id={self.id}, text={self.text[:50]}..., metadata={self.metadata_})'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d26f6",
   "metadata": {},
   "source": [
    "Define database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaccceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import select\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker\n",
    "\n",
    "class RAGDatabase():\n",
    "  def __init__(self, username='username', password='password'):\n",
    "\n",
    "    \"\"\"\n",
    "    Initializes a connection to the database\n",
    "    \"\"\"\n",
    "\n",
    "    self.username = username\n",
    "    self.password = username\n",
    "    self.DB_URL = f'postgresql+asyncpg://{username}:{password}@localhost:5432/edgerag_db'\n",
    "    self.engine = create_async_engine(self.DB_URL)\n",
    "    self.Session = async_sessionmaker(self.engine, expire_on_commit=False)\n",
    "    if self.Session:\n",
    "      print(\"Successfully initialized connection\")\n",
    "\n",
    "  async def db_create(self):\n",
    "      async with self.engine.begin() as conn:\n",
    "          await conn.run_sync(Base.metadata.create_all)\n",
    "\n",
    "  async def __reset_db(self):\n",
    "    async with self.engine.begin() as conn:\n",
    "        await conn.run_sync(Base.metadata.drop_all)\n",
    "        await conn.run_sync(Base.metadata.create_all)\n",
    "\n",
    "  async def add_document_to_vector_db(self, doc: RAGDocument): # TODO: This function needs a safety guardrail to ensure duplicates don't get added to the database!\n",
    "\n",
    "    \"\"\"\n",
    "    Adds a document to the database\n",
    "    \"\"\"\n",
    "\n",
    "    doc_chunks = doc.chunks\n",
    "    commit_chunks = []\n",
    "    for doc_chunk in doc_chunks:\n",
    "        commit_chunks.append({\n",
    "            'text': doc_chunk['text'],\n",
    "            'vector': doc_chunk['embedding'],\n",
    "            'metadata_': doc.metadata\n",
    "        })\n",
    "    async with self.Session() as db:\n",
    "        for commit_chunk in commit_chunks:\n",
    "            db.add(VectorDocument(**commit_chunk))\n",
    "        await db.commit()\n",
    "\n",
    "  async def vector_search(self, query_vector, top_k=3):\n",
    "\n",
    "    \"\"\"\n",
    "    Searches the database for documents that are cosine similar to the query vector\n",
    "    \"\"\"\n",
    "\n",
    "    async with self.Session() as db:\n",
    "        query = (\n",
    "            select(VectorDocument.text, VectorDocument.metadata_, VectorDocument.vector.cosine_distance(query_vector).label('distance'))\n",
    "            .order_by('distance')\n",
    "            .limit(top_k)\n",
    "        )\n",
    "        res = await db.execute(query)\n",
    "        result = []\n",
    "        for text, metadata, distance, in res:\n",
    "            result.append({\n",
    "                'text': text,\n",
    "                'metadata': metadata,\n",
    "                'score': 1 - distance\n",
    "            })\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df6ac1a",
   "metadata": {},
   "source": [
    "Define RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5107958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RAG():\n",
    "  def __init__(self, api_searcher, text_model, embedding_model, keyword_model):\n",
    "\n",
    "    \"\"\"\n",
    "    To make this system as modular as possible,\n",
    "    I'm defining these parameters as functions:\n",
    "    - api_searcher: a function that contains a method scrape(query, results=self.k) and returns a list of Document objects\n",
    "    - text_model: a function that contains a method generate(content) and returns a response text string\n",
    "    - embedding_model: a function that contains a method encode(text) and returns a vector of size VECTOR_SIZE\n",
    "    - keyword_model: a function that contains a method extract(query) and returns a keyword text string\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle non-variables\n",
    "    if not api_searcher or not text_model or not embedding_model or not keyword_model:\n",
    "       print(\"Please check parameters!\")\n",
    "\n",
    "    # Initialize attributes\n",
    "    self.k = 5\n",
    "    self.api_searcher = api_searcher\n",
    "    self.text_model = text_model\n",
    "    self.embedding_model = embedding_model\n",
    "    self.keyword_model = keyword_model\n",
    "\n",
    "    # Initialize database\n",
    "    self.database = RAGDatabase()\n",
    "\n",
    "    # Initialize prompts\n",
    "    self.system_prompt = \"\"\"\n",
    "    You are an AI assistant that answers questions about documents in your knowledge base.\n",
    "    \"\"\"\n",
    "\n",
    "    self.rag_prompt = \"\"\"\n",
    "    Use the following pieces of context to answer the user question.\n",
    "    You must only use the facts from the context to answer.\n",
    "    If the answer cannot be found in the context, say that you don't have enough information to answer the question and provide any relevant facts found in the context.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    User Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "\n",
    "  def _API_search(self, query):\n",
    "\n",
    "    \"\"\"\n",
    "    Calls an API to scrape for relevant texts\n",
    "    \"\"\"\n",
    "\n",
    "    return self.api_searcher.scrape(query, results=self.k)\n",
    "\n",
    "  def _get_keyword(self, query):\n",
    "\n",
    "    \"\"\"\n",
    "    Extracts a keywrod from a query\n",
    "    \"\"\"\n",
    "\n",
    "    return self.keyword_model.extract(query)\n",
    "\n",
    "  def _get_embedding(self, text):\n",
    "\n",
    "     \"\"\"\n",
    "     Gets embedding from text\n",
    "     \"\"\"\n",
    "\n",
    "     return self.embedding_model.encode(text)\n",
    "\n",
    "  async def _generate_text(self, content):\n",
    "\n",
    "     \"\"\"\n",
    "     Generates text\n",
    "     \"\"\"\n",
    "\n",
    "     return await self.text_model.generate(content)\n",
    "\n",
    "  async def ask(self, question, add=False, print_output=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Main function call: Asks the RAG system a question\n",
    "    \"\"\"\n",
    "\n",
    "    if add:\n",
    "      keyword = self._get_keyword(question)\n",
    "      await self.add(keyword)\n",
    "\n",
    "    query_vector = self._get_embedding(question)\n",
    "    top_chunks = await self.database.vector_search(query_vector, top_k=self.k)\n",
    "    context = '\\n\\n---\\n\\n'.join([chunk['text'] for chunk in top_chunks]) + '\\n\\n---'\n",
    "    user_message = self.rag_prompt.format(context=context, question=question)\n",
    "    messages = self.system_prompt + \" \" + user_message\n",
    "    response = await self._generate_text(messages)\n",
    "\n",
    "    if print_output:\n",
    "      print(\"Response:\", response)\n",
    "\n",
    "    return response\n",
    "\n",
    "  async def add(self, query):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function: Search the API (not database) and add documents to the database\n",
    "    \"\"\"\n",
    "\n",
    "    docs = self._API_search(query)\n",
    "    for doc in docs:\n",
    "      doc.chunkify(embedding_model=self.embedding_model)\n",
    "      await self.database.add_document_to_vector_db(doc)\n",
    "    print(len(docs), \"(potentially unique?) documents added\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237d732",
   "metadata": {},
   "source": [
    "Initialize wrappers & actual objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca6b4259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Wikipedia\n",
      "Initialized Gemini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 716.57it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 628.52it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized KeyBERT\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import wikipedia\n",
    "from keybert import KeyBERT\n",
    "\n",
    "\"\"\"\n",
    "We have to define the following:\n",
    "- api_searcher\n",
    "- text_model\n",
    "- embedding_model\n",
    "- keyword_model\n",
    "\"\"\"\n",
    "\n",
    "# Example: api_searcher through Python's wikipedia module\n",
    "class wikipedia_searcher():\n",
    "  def __init__(self, wk):\n",
    "    self.wk = wk\n",
    "    print(\"Initialized Wikipedia\")\n",
    "  def scrape(self, query, results):\n",
    "    titles = self.wk.search(query, results=results)\n",
    "    pages = [self.wk.page(title, auto_suggest=False) for title in titles]\n",
    "    return [RAGDocument(text=page.content, metadata={'URL': page.url, 'title': page.title}) for page in pages]\n",
    "\n",
    "api_searcher = wikipedia_searcher(wikipedia)\n",
    "\n",
    "# Example: text_model through Gemini\n",
    "class gemini_model():\n",
    "  def __init__(self, api_key):\n",
    "    self.client = genai.Client(api_key=api_key)\n",
    "    print(\"Initialized Gemini\")\n",
    "  async def generate(self, content):\n",
    "    if DEBUG:\n",
    "      return \"You are currently in debug mode so no Gemini API credits will be used\"\n",
    "    response = self.client.models.generate_content(\n",
    "      model=\"gemini-2.5-flash\",\n",
    "      contents = content\n",
    "    )\n",
    "    return ' '.join(response.text.split())\n",
    "\n",
    "text_model = gemini_model(api_key=API_KEY)\n",
    "\n",
    "# Example: embedding_model through SentenceTransformer # NOTE: Already has the encode method, thus, no need to create a wrapper around it.\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example: keyword model through KeyBERT\n",
    "class keybert_model():\n",
    "  def __init__(self):\n",
    "    self.keyword_model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "    print(\"Initialized KeyBERT\")\n",
    "  def extract(self, query):\n",
    "    return self.keyword_model.extract_keywords(query)[0][0]\n",
    "\n",
    "keyword_model = keybert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf25847",
   "metadata": {},
   "source": [
    "Finally, our RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0e8ad85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized connection\n",
      "5 (potentially unique?) documents added\n",
      "Response: Barack Hussein Obama II (born August 4, 1961) is an American politician who served as the 44th president of the United States from 2009 to 2017. He is a member of the Democratic Party and was the first African American president. Before his presidency, Obama served as a U.S. senator representing Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004. He was born in Honolulu, Hawaii, and graduated from Columbia University in 1983 with a Bachelor of Arts degree in political science. He later enrolled in Harvard Law School in 1988, where he was the first black president of the Harvard Law Review. Obama worked as a community organizer in Chicago, became a civil rights attorney, and taught constitutional law at the University of Chicago Law School from 1992 to 2004. He was awarded the 2009 Nobel Peace Prize for efforts in international diplomacy. During his presidency, his administration responded to the 2008 financial crisis, reformed health care with the Affordable Care Act, ended the Iraq War, ordered the raid that killed Osama bin Laden, and appointed Supreme Court justices Sonia Sotomayor and Elena Kagan. In his second term, he advocated for gun control, took steps to combat climate change, initiated sanctions against Russia, ordered military intervention in Iraq against ISIL, negotiated a nuclear agreement with Iran, and normalized relations with Cuba.\n"
     ]
    }
   ],
   "source": [
    "rag = RAG(api_searcher=api_searcher,\n",
    "          text_model=text_model,\n",
    "          embedding_model=embedding_model,\n",
    "          keyword_model=keyword_model)\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "await rag.database.db_create()\n",
    "\n",
    "response = await rag.ask(\"Who is Barack Obama?\", add=True, print_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EdgeRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
